{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Introduction \n",
        "\n",
        "In this notebook, the analysis are made on SMOTE, BorderlineSMOTE, Hybrid SMOTE, and Hybrid BorderlineSMOTE. \n",
        "\n",
        "I got results for 3 different imbalanced datasets using the default method for SMOTE and BorderlineSMOTE. In addition, I used a custom sampling strategy by creating a custom dictionary and got results for 6 different datasets.\n"
      ],
      "metadata": {
        "id": "XcxvKvH4AKhl"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WpQkzaLRwr-b",
        "outputId": "5e9423c8-512b-45ae-8de7-ec9528f1009e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "#SMOTEs\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from imblearn.over_sampling import BorderlineSMOTE\n",
        "import math\n",
        "\n",
        "# IMPORTS FOR TRAINING\n",
        "from sklearn.utils import shuffle\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "#from xgboost import XGBClassifier\n",
        "#from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
        "#from sklearn.neighbors import KNeighborsClassifier\n",
        "#from sklearn import linear_model\n",
        "#from sklearn.linear_model import RidgeClassifier, LogisticRegression\n",
        "#from sklearn import svm\n",
        "\n",
        "#RESULTS\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score, classification_report, confusion_matrix\n",
        "\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Augmentation Methods\n",
        "\n",
        "Both [SMOTE](https://imbalanced-learn.org/stable/references/generated/imblearn.over_sampling.SMOTE.html) and [BorderlineSMOTE](https://imbalanced-learn.org/stable/references/generated/imblearn.over_sampling.BorderlineSMOTE.html) have package in imblearn library. They are basically designed for imbalanced binary data sets to increase the amount of lower label to bigger one. It is working on numerical representations so we will be using our vectorized text data bring back from vectorized versions which we created with vectorizer notebook.\n",
        "\n",
        "In ctweet data set, we have 3 labels which it is not allowed for default sampling_strategy in SMOTE or BorderlineSMOTE. To solve the problem, we use dictionary to increase the amount of 2 other lower labels to biggest one like it is a Hybrid version.\n",
        "_____"
      ],
      "metadata": {
        "id": "C00SQoufBagl"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "OKu3WMWE2QLY"
      },
      "outputs": [],
      "source": [
        "def HybridSMOTE(x, y):\n",
        "    smote = SMOTE(sampling_strategy = new_dict, random_state = 42)\n",
        "    x, y = smote.fit_resample(x, y)\n",
        "\n",
        "    return x, y\n",
        "\n",
        "def HybridBORDER(x, y):\n",
        "    smote = BorderlineSMOTE(sampling_strategy = new_dict, random_state = 42)\n",
        "    x, y = smote.fit_resample(x, y)\n",
        "\n",
        "    return x, y\n",
        "\n",
        "def smote(x, y):\n",
        "    smote = SMOTE(sampling_strategy= 1, random_state=42)\n",
        "    x, y = smote.fit_resample(x, y)\n",
        "\n",
        "    return x, y\n",
        "\n",
        "def BlSmote(x, y):\n",
        "    sm = BorderlineSMOTE(sampling_strategy=1, random_state = 42)\n",
        "    x, y = sm.fit_resample(x, y)\n",
        "\n",
        "    return x, y"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data sets\n",
        "\n",
        "We have 6 data sets to test our methods. Every data set has function to call easily, you just need to change location.\n",
        "\n",
        "To understand why .iloc is used exactly, you can check without using it. While the vectorized version is saved on .csv file in vectorizer notebook, there is a column added which we do not interested.\n",
        "\n",
        "In some functions you will see [:150000] or [:20000] while assignment. It is about not losing the matches for labels and data which we decreased in vectorizing phase.\n",
        "______"
      ],
      "metadata": {
        "id": "x3ig3SjVEROU"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "XMmVjNZLxIVi"
      },
      "outputs": [],
      "source": [
        "def get_reddit():\n",
        "  b = pd.read_csv(\"/content/drive/MyDrive/TUBITAK_TASK1/encoded_texts/reddit/encoded_reddit150000_train.csv\").iloc[:, 1:]\n",
        "  word_vectors_train = pd.DataFrame(b)\n",
        "\n",
        "  b = pd.read_csv(\"/content/drive/MyDrive/TUBITAK_TASK1/encoded_texts/reddit/encoded_reddit20000_test.csv\").iloc[:, 1:]\n",
        "  word_vectors_test = pd.DataFrame(b)\n",
        "\n",
        "  train_data = pd.read_csv(\"/content/drive/MyDrive/TUBITAK_TASK1/data/reddit/train.csv\")\n",
        "  test_data = pd.read_csv('/content/drive/MyDrive/TUBITAK_TASK1/data/reddit/test.csv')\n",
        "\n",
        "  train_labels = train_data[\"Y\"]\n",
        "  test_labels = test_data[\"Y\"]\n",
        "\n",
        "  x_train = word_vectors_train[:150000]\n",
        "  y_train = train_labels[:150000]\n",
        "\n",
        "  x_test = word_vectors_test[:20000]\n",
        "  y_test = test_labels[:20000]\n",
        "\n",
        "  del word_vectors_train\n",
        "  del word_vectors_test\n",
        "  del train_data\n",
        "  del test_data\n",
        "\n",
        "  return x_train, y_train, x_test, y_test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "Of8Ouiwhz_Jy"
      },
      "outputs": [],
      "source": [
        "def get_stweet():\n",
        "  b = pd.read_csv(\"/content/drive/MyDrive/TUBITAK_TASK1/encoded_texts/stweet/encoded_stweet150000_train.csv\").iloc[:, 1:]\n",
        "  word_vectors_train = pd.DataFrame(b)\n",
        "\n",
        "  b = pd.read_csv(\"/content/drive/MyDrive/TUBITAK_TASK1/encoded_texts/stweet/encoded_stweet20000_test.csv\").iloc[:, 1:]\n",
        "  word_vectors_test = pd.DataFrame(b)\n",
        "\n",
        "  train_data = pd.read_csv(\"/content/drive/MyDrive/TUBITAK_TASK1/data/stweet/train.csv\")\n",
        "  test_data = pd.read_csv('/content/drive/MyDrive/TUBITAK_TASK1/data/stweet/test.csv')\n",
        "\n",
        "  train_labels = train_data[\"Y\"]\n",
        "  test_labels = test_data[\"Y\"]\n",
        "\n",
        "  x_train = word_vectors_train[:150000]\n",
        "  y_train = train_labels[:150000]\n",
        "\n",
        "  x_test = word_vectors_test[:20000]\n",
        "  y_test = test_labels[:20000]\n",
        "\n",
        "  del word_vectors_train\n",
        "  del word_vectors_test\n",
        "  del train_data\n",
        "  del test_data\n",
        "\n",
        "  return x_train, y_train, x_test, y_test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "50Uzd5rlwvTu"
      },
      "outputs": [],
      "source": [
        "def get_ctweet():\n",
        "  b = pd.read_csv(\"/content/drive/MyDrive/TUBITAK_TASK1/encoded_texts/ctweet/encoded_ctweet_train.csv\").iloc[:, 1:]\n",
        "  word_vectors_train = pd.DataFrame(b)\n",
        "\n",
        "  b = pd.read_csv(\"/content/drive/MyDrive/TUBITAK_TASK1/encoded_texts/ctweet/encoded_ctweet_test.csv\").iloc[:, 1:]\n",
        "  word_vectors_test = pd.DataFrame(b)\n",
        "\n",
        "  train_data = pd.read_csv(\"/content/drive/MyDrive/TUBITAK_TASK1/data/ctweet/train.csv\")\n",
        "  test_data = pd.read_csv(\"/content/drive/MyDrive/TUBITAK_TASK1/data/ctweet/test.csv\")\n",
        "\n",
        "  train_labels = train_data[\"Y\"]\n",
        "  test_labels = test_data[\"Y\"]\n",
        "\n",
        "  x_train = word_vectors_train\n",
        "  y_train = train_labels\n",
        "\n",
        "  x_test = word_vectors_test\n",
        "  y_test = test_labels\n",
        "\n",
        "  return x_train, y_train, x_test, y_test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "mpd3hust0x1Z"
      },
      "outputs": [],
      "source": [
        "def get_sarcasm():\n",
        "  b = pd.read_csv(\"/content/drive/MyDrive/TUBITAK_TASK1/encoded_texts/sarcasm/encoded_sarcasm_train.csv\").iloc[:, 1:]\n",
        "  word_vectors_train = pd.DataFrame(b)\n",
        "\n",
        "  b = pd.read_csv(\"/content/drive/MyDrive/TUBITAK_TASK1/encoded_texts/sarcasm/encoded_sarcasm_test.csv\").iloc[:, 1:]\n",
        "  word_vectors_test = pd.DataFrame(b)\n",
        "\n",
        "  train_data = pd.read_csv(\"/content/drive/MyDrive/TUBITAK_TASK1/data/sarcasm/train.csv\")\n",
        "  test_data = pd.read_csv('/content/drive/MyDrive/TUBITAK_TASK1/data/sarcasm/test.csv')\n",
        "\n",
        "  train_labels = train_data[\"Y\"]\n",
        "  test_labels = test_data[\"Y\"]\n",
        "\n",
        "  x_train = word_vectors_train\n",
        "  y_train = train_labels\n",
        "\n",
        "  x_test = word_vectors_test\n",
        "  y_test = test_labels\n",
        "\n",
        "  return x_train, y_train, x_test, y_test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "PmSRT1xH0_tB"
      },
      "outputs": [],
      "source": [
        "def get_toxic():\n",
        "  b = pd.read_csv(\"/content/drive/MyDrive/TUBITAK_TASK1/encoded_texts/toxic/encoded_toxic_train.csv\").iloc[:, 1:]\n",
        "  word_vectors_train = pd.DataFrame(b)\n",
        "\n",
        "  b = pd.read_csv(\"/content/drive/MyDrive/TUBITAK_TASK1/encoded_texts/toxic/encoded_toxic_test.csv\").iloc[:, 1:]\n",
        "  word_vectors_test = pd.DataFrame(b)\n",
        "\n",
        "  train_data = pd.read_csv(\"/content/drive/MyDrive/TUBITAK_TASK1/data/toxic/train.csv\")\n",
        "  test_data = pd.read_csv('/content/drive/MyDrive/TUBITAK_TASK1/data/toxic/test.csv')\n",
        "\n",
        "  train_labels = train_data[\"Y\"]\n",
        "  test_labels = test_data[\"Y\"]\n",
        "\n",
        "  x_train = word_vectors_train\n",
        "  y_train = train_labels\n",
        "\n",
        "  x_test = word_vectors_test\n",
        "  y_test = test_labels\n",
        "\n",
        "  return x_train, y_train, x_test, y_test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "hNIQNsyx1H2p"
      },
      "outputs": [],
      "source": [
        "def get_food():\n",
        "  b = pd.read_csv(\"/content/drive/MyDrive/TUBITAK_TASK1/encoded_texts/food/encoded_food_train.csv\").iloc[:, 1:]\n",
        "  word_vectors_train = pd.DataFrame(b)\n",
        "\n",
        "  b = pd.read_csv(\"/content/drive/MyDrive/TUBITAK_TASK1/encoded_texts/food/encoded_food20000_test.csv\").iloc[:, 1:]\n",
        "  word_vectors_test = pd.DataFrame(b)\n",
        "\n",
        "  train_data = pd.read_csv(\"/content/drive/MyDrive/TUBITAK_TASK1/data/food/train.csv\")\n",
        "  test_data = pd.read_csv('/content/drive/MyDrive/TUBITAK_TASK1/data/food/test.csv')\n",
        "\n",
        "  train_labels = train_data[\"Y\"]\n",
        "  test_labels = test_data[\"Y\"]\n",
        "\n",
        "  x_train = word_vectors_train\n",
        "  y_train = train_labels\n",
        "\n",
        "  x_test = word_vectors_test[:20000]\n",
        "  y_test = test_labels[:20000]\n",
        "\n",
        "  return x_train, y_train, x_test, y_test"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train the model\n",
        "I used MLPClassifier to get possible highest scores in shortest time. Both train and metrics are functionalized.\n",
        "_____"
      ],
      "metadata": {
        "id": "tNF6C8D7F8KV"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "3G8NoPagAG5z"
      },
      "outputs": [],
      "source": [
        "def train_model(x_train, y_train, x_test):\n",
        "  clf = MLPClassifier(max_iter=10, hidden_layer_sizes = (50,50)).fit(x_train, y_train)\n",
        "  y_pred = clf.predict(x_test)\n",
        "  return y_pred"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "jawyl9cID_te"
      },
      "outputs": [],
      "source": [
        "def print_metrics(y_test, y_pred):\n",
        "  print('Precision: %.4f' % precision_score(y_test, y_pred, average='weighted'))\n",
        "  print('Recall: %.4f' % recall_score(y_test, y_pred, average='weighted'))\n",
        "  print('Accuracy: %.4f' % accuracy_score(y_test, y_pred))\n",
        "  print('F1 Score: %.4f' % f1_score(y_test, y_pred, average='weighted'))\n",
        "  print(classification_report(y_test, y_pred))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "OrFNQ8OcQmaL"
      },
      "outputs": [],
      "source": [
        "x_train, y_train, x_test, y_test = get_sarcasm()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mI306aZ71jMC",
        "outputId": "7a39f2de-26dd-40b6-cf0a-ad315a81ed3c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{0: 10479, 1: 9554}"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ],
      "source": [
        "# CREATING DICTIONARY TO BE ABLE TO AUGMENT NECESSARY CLASSES\n",
        "\n",
        "df_c = pd.DataFrame(y_train)\n",
        "value_counts = df_c.value_counts()\n",
        "dictionary = dict()\n",
        "for (i,), j in value_counts.items():\n",
        "  dictionary[i] = j\n",
        "\n",
        "dictionary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5hJYcOIZ1oTh",
        "outputId": "3508152a-df6d-4581-9800-04fdb88adb8b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{0: 20958, 1: 19108}"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ],
      "source": [
        "# ASSIGN NEW DICTIONARY TO ADJUST AMOUNT OF AUGMENTATION\n",
        "\n",
        "augment_zero = int(dictionary[0] * 2)\n",
        "augment_one = int(dictionary[1] * 2)\n",
        "#augment_two = int(dictionary[2] * 2)\n",
        "\n",
        "new_dict = {0 : augment_zero, 1: augment_one}\n",
        "new_dict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "nU3jAfYv2OLM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "936efb06-d622-4d74-8757-785e50f9b556"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/imblearn/utils/_validation.py:300: UserWarning: After over-sampling, the number of samples (20958) in class 0 will be larger than the number of samples in the majority class (class #0 -> 10479)\n",
            "  f\"After over-sampling, the number of samples ({n_samples})\"\n",
            "/usr/local/lib/python3.7/dist-packages/imblearn/utils/_validation.py:300: UserWarning: After over-sampling, the number of samples (19108) in class 1 will be larger than the number of samples in the majority class (class #0 -> 10479)\n",
            "  f\"After over-sampling, the number of samples ({n_samples})\"\n"
          ]
        }
      ],
      "source": [
        "#AUGMENTATION\n",
        "x_new, y_new = HybridBORDER(x_train, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "q2GLo4o72PnJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "73efb875-9a57-48e8-fb47-88ccc1dbabc1"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{0: 20958, 1: 19108}"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ],
      "source": [
        "# Check the augmented label count\n",
        "\n",
        "df_c = pd.DataFrame(y_new)\n",
        "value_counts = df_c.value_counts()\n",
        "dictionary = dict()\n",
        "for (i,), j in value_counts.items():\n",
        "  dictionary[i] = j\n",
        "\n",
        "dictionary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "hw0x1sTLTb2V"
      },
      "outputs": [],
      "source": [
        "from sklearn.utils import shuffle\n",
        "x_new, y_new = shuffle(x_new, y_new)\n",
        "x_test, y_test = shuffle(x_test, y_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "XIUCjF3-4PL5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "beb0120b-1027-4067-c6b8-26d2073989ae"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Precision: 0.8581\n",
            "Recall: 0.8576\n",
            "Accuracy: 0.8576\n",
            "F1 Score: 0.8576\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.88      0.85      0.86      4506\n",
            "           1       0.84      0.87      0.85      4080\n",
            "\n",
            "    accuracy                           0.86      8586\n",
            "   macro avg       0.86      0.86      0.86      8586\n",
            "weighted avg       0.86      0.86      0.86      8586\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (10) reached and the optimization hasn't converged yet.\n",
            "  ConvergenceWarning,\n"
          ]
        }
      ],
      "source": [
        "#Training augmented data\n",
        "y_pred = train_model(x_new, y_new, x_test)\n",
        "print_metrics(y_test, y_pred)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "collapsed_sections": [],
      "name": "data_augmentation.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMY3gTqLjJ7bQCLh1Wx3Bx6"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}